---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hello, I am Jaeyeul Kim, a Ph.D. student in the Department of Electrical Engineering and Computer Science (EECS) at DGIST in South Korea.
I am conducting research on autonomous driving perception under the supervision of Professor Sunghoon Im at the DGIST Computer Vision Lab.
Specifically, I have worked on LiDAR-based segmentation, detection, tracking, and forecasting.
My current research interest is in autonomous vehicle planning.


Education & Research Experience
======
- **Visiting Researcher**<br>
Robotics Institute, Carnegie Mellon University (**CMU**), Pittsburgh, US.<br>
Apr 2024 - Oct 2024

- **Ph.D. Course in Electrical Engineering and Computer Science**<br>
Daegu Gyeongbuk Institute of Science and Technology (**DGIST**), South Korea.<br>
Aug 2021 - Present

- **M.S. in Automotive Engineering**<br>
Seoul National University of Science and Technology (**SEOULTECH**), South Korea.<br>
Aug 2018 - Aug 2021

- **B.S. in Mechanical and Automotive Engineering**<br>
Seoul National University of Science and Technology (**SEOULTECH**), South Korea.<br>
Mar 2012 - Aug 2018


<!--
Publications
======
<style>
    .publication {
        display: flex;
        align-items: center;
        margin-bottom: -20px;
    }
    .publication img {
        margin-right: 20px;
        width: 150px; /* 이미지 크기를 적절히 조절하세요 */
        height: auto;
    }
    .publication-info {
        max-width: calc(100% - 170px); /* 이미지와 마진을 고려한 너비 조절 */
    }
</style>

<div class="publication">
    <img src='/images/500x300.png' alt="Publication Image">
    <div class="publication-info">
        <h3>Rethinking LiDAR Domain Generalization: Single Source as Multiple Density Domains</h3>
        <p><strong>Jaeyeul Kim*</strong>, Jungwan Woo*, Jeonghoon Kim, Sunghoon Im<br>
        European Conference on Computer Vision (<strong>ECCV</strong>), 2024. (Accepted)</p>
    </div>
</div>

<div class="publication">
    <img src='/images/500x300.png' alt="Publication Image">
    <div class="publication-info">
        <h3>Density-aware Domain Generalization for LiDAR Semantic Segmentation</h3>
        <p><strong>Jaeyeul Kim*</strong>, Jungwan Woo*, Ukcheol Shin, Jean Oh, Sunghoon Im<br>
        IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>), 2024. (Accepted)</p>
    </div>
</div>

<div class="publication">
    <img src='/images/500x300.png' alt="Publication Image">
    <div class="publication-info">
        <h3>Automatic Extrinsic Calibration of a Camera and a 2D LiDAR with Point-Line Correspondences</h3>
        <p><strong>Jaeyeul Kim</strong> and Jongeun Ha<br>
        <strong>IEEE Access</strong>, 2023.</p>
    </div>
</div>

<div class="publication">
    <img src='/images/500x300.png' alt="Publication Image">
    <div class="publication-info">
        <h3>Motion Forecasting via Coordinate Transformations and Object Trajectory Modifications</h3>
        <p>Jungwan Woo*, <strong>Jaeyeul Kim*</strong>, Sunghoon Im<br>
        CVPR workshop on Autonomous Driving (<strong>CVPRw</strong>), 2023. (Technical report)</p>
    </div>
</div>

<div class="publication">
    <img src='/images/500x300.png' alt="Publication Image">
    <div class="publication-info">
        <h3>LiDAR 3D Object Detection via Self-Training and Knowledge Distillation</h3>
        <p>Jungwan Woo*, <strong>Jaeyeul Kim*</strong>, Sunghoon Im<br>
        ECCV workshop on 3D Perception for Autonomous Driving (<strong>ECCVw</strong>), 2022. (Technical report)</p>
    </div>
</div>

<div class="publication">
    <img src='/images/500x300.png' alt="Publication Image">
    <div class="publication-info">
        <h3>Weakly Supervised Foreground Object Detection Network Using Background Model Image</h3>
        <p><strong>Jaeyeul Kim</strong> and Jongeun Ha<br>
        <strong>IEEE Access</strong>, 2022.</p>
    </div>
</div>

<div class="publication">
    <img src='/images/500x300.png' alt="Publication Image">
    <div class="publication-info">
        <h3>RVMOS: Range-View Moving Object Segmentation Leveraged by Semantic and Motion Features</h3>
        <p><strong>Jaeyeul Kim*</strong>, Jungwan Woo*, Sunghoon Im<br>
        IEEE Robotics and Automation Letters (<strong>RA-L / IROS</strong>), 2022. </p>
    </div>
</div>

<div class="publication">
    <img src='/images/500x300.png' alt="Publication Image">
    <div class="publication-info">
        <h3>Spatio-temporal data augmentation for visual surveillance</h3>
        <p><strong>Jaeyeul Kim</strong> and Jongeun Ha<br>
        <strong>IEEE Access</strong>, 2021.</p>
    </div>
</div>

<div class="publication">
    <img src='/images/500x300.png' alt="Publication Image">
    <div class="publication-info">
        <h3>Generation of Background Model Image Using Foreground Model</h3>
        <p><strong>Jaeyeul Kim</strong> and Jongeun Ha<br>
        <strong>IEEE Access</strong>, 2021.</p>
    </div>
</div>

<div class="publication">
    <img src='/images/500x300.png' alt="Publication Image">
    <div class="publication-info">
        <h3>Extrinsic Calibration of a Camera and a 2D LiDAR using a Dummy Camera with IR Cut Filter Removed</h3>
        <p><strong>Jaeyeul Kim</strong> and Jongeun Ha<br>
        <strong>IEEE Access</strong>, 2020.</p>
    </div>
</div>

<div class="publication">
    <img src='/images/500x300.png' alt="Publication Image">
    <div class="publication-info">
        <h3>Foreground Objects Detection Using a Fully Convolutional Network with a Background Model Image and Multiple Original Images</h3>
        <p><strong>Jaeyeul Kim</strong> and Jongeun Ha<br>
        <strong>IEEE Access</strong>, 2020.</p>
    </div>
</div>

<div class="publication">
    <img src='/images/500x300.png' alt="Publication Image">
    <div class="publication-info">
        <h3>Automatic Extrinsic Calibration of a Camera and Laser Range Finder using Constraints Fusion</h3>
        <p><strong>Jaeyeul Kim</strong> and Jongeun Ha<br>
        International Conference on Control, Automation and Systems (<strong>ICCAS</strong>), 2019.</p>
    </div>
</div>

<div class="publication">
    <img src='/images/500x300.png' alt="Publication Image">
    <div class="publication-info">
        <h3>Extrinsic Calibration of Camera and Laser Range Finder using a Dummy Camera without IR Cut Filter</h3>
        <p><strong>Jaeyeul Kim</strong> and Jongeun Ha<br>
        International Conference on Control, Automation and Systems (<strong>ICCAS</strong>), 2018.</p>
    </div>
</div>
-->


Publications
======
<img src='/images/500x300.png' style="width: 300px; height: 200px; margin-bottom: -20px;">

### Rethinking LiDAR Domain Generalization: Single Source as Multiple Density Domains
**Jaeyeul Kim\***, Jungwan Woo\*, Jeonghoon Kim, Sunghoon Im<br>
European Conference on Computer Vision (**ECCV**), 2024. (Accepted)

---

<img src='/images/500x300.png' style="width: 300px; height: 200px; margin-bottom: -20px;">

### Density-aware Domain Generalization for LiDAR Semantic Segmentation
**Jaeyeul Kim\***, Jungwan Woo\*, Ukcheol Shin, Jean Oh, Sunghoon Im<br>
IEEE/RSJ International Conference on Intelligent Robots and Systems (**IROS**), 2024. (Accepted)

---

<img src='/images/Automatic.png' style="width: 300px; height: 200px; margin-bottom: -20px;">

### Automatic Extrinsic Calibration of a Camera and a 2D LiDAR with Point-Line Correspondences
**Jaeyeul Kim** and Jongeun Ha<br>
**IEEE Access**, 2023.

---

<img src='/images/CVPRw.png' style="width: 300px; height: 200px; margin-bottom: -20px;">

### Motion Forecasting via Coordinate Transformations and Object Trajectory Modifications
Jungwan Woo\*, **Jaeyeul Kim\***, Sunghoon Im<br>
CVPR workshop on Autonomous Driving (**CVPRw**), 2023. (Technical report)

---

<img src='/images/ECCVw.png' style="width: 300px; height: 200px; margin-bottom: -20px;">

### LiDAR 3D Object Detection via Self-Training and Knowledge Distillation
Jungwan Woo\*, **Jaeyeul Kim\***, Sunghoon Im<br>
ECCV workshop on 3D Perception for Autonomous Driving (**ECCVw**), 2022. (Technical report)

---


<img src='/images/Weakly.png' style="width: 300px; height: 200px; margin-bottom: -20px;">

### Weakly Supervised Foreground Object Detection Network Using Background Model Image
**Jaeyeul Kim** and Jongeun Ha<br>
**IEEE Access**, 2022.

---

<img src='/images/rvmos.png' style="width: 300px; height: 200px; margin-bottom: -20px;">

### RVMOS: Range-View Moving Object Segmentation Leveraged by Semantic and Motion Features
**Jaeyeul Kim\***, Jungwan Woo\*, Sunghoon Im<br>
 IEEE Robotics and Automation Letters (**RA-L / IROS**), 2022. 

---

### Spatio-temporal data augmentation for visual surveillance
**Jaeyeul Kim** and Jongeun Ha<br>
**IEEE Access**, 2021.

---

### Generation of Background Model Image Using Foreground Model
**Jaeyeul Kim** and Jongeun Ha<br>
**IEEE Access**, 2021.

---

<img src='/images/Exrinsic_2020.png' style="width: 300px; height: 200px; margin-bottom: -20px;">

### Extrinsic Calibration of a Camera and a 2D LiDAR using a Dummy Camera with IR Cut Filter Removed
**Jaeyeul Kim** and Jongeun Ha<br>
**IEEE Access**, 2020.

---

### Foreground Objects Detection Using a Fully Convolutional Network with a Background Model Image and Multiple Original Images
**Jaeyeul Kim** and Jongeun Ha<br>
**IEEE Access**, 2020.

---

### Automatic Extrinsic Calibration of a Camera and Laser Range Finder using Constraints Fusion
**Jaeyeul Kim** and Jongeun Ha<br>
International Conference on Control, Automation and Systems (**ICCAS**), 2019.

---

### Extrinsic Calibration of Camera and Laser Range Finder using a Dummy Camera without IR Cut Filter
**Jaeyeul Kim** and Jongeun Ha<br>
International Conference on Control, Automation and Systems (**ICCAS**), 2018.

---

<div style="margin-bottom: 20px;"></div>

Scholarships & Awards
======
- **Winner**, Argoverse LiDAR Scene Flow Challenge at CVPR WAD, 2024.
- **DGIST Post-Graduate Research Abroad Award**, 2024.
- **1st place**, Autonomous Driving A.I challenge (organized by MOLIT 국토교통부), 2023.
- **Honorable Mention**, Argoverse Forecasting Challenge at CVPR WAD, 2023.
- **Best Robot Vision Paper Award**, Asian Federation of Computer Vision (AFCV), 2023.
- **2nd place**, Autonomous Driving A.I challenge (organized by MOLIT 국토교통부), 2022.
- **3rd place**, ECCV workshop on 3D Perception for Autonomous Driving, 2022.
- **DGIST Presidential Fellowship**, 2021.
- **SEOULTECH Graduate School Admission Scholarship**, 2018.
- **Bronze Award**, SoC Robot War-HURO Competition (organized by KAIST SDIA), 2018.
- **Bronze Award**, SoC Robot War-HURO Competition (organized by KAIST SDIA), 2017.

<!-- 
<ul style="margin: 0; padding: 0; list-style-position: inside;">
  <li style="line-height: 1.2;"><span style="font-size: 16px; color: black"><strong>Winner</strong>, Argoverse LiDAR Scene Flow Challenge at CVPR WAD, 2024.</span></li>
  <li style="line-height: 1.2;"><span style="font-size: 16px; color: black"><strong>DGIST Post-Graduate Research Abroad Award</strong>, 2024.</span></li>
  <li style="line-height: 1.2;"><span style="font-size: 16px; color: black"><strong>1st place</strong>, Autonomous Driving A.I challenge (organized by MOLIT 국토교통부), 2023.</span></li>
  <li style="line-height: 1.2;"><span style="font-size: 16px; color: black"><strong>Honorable Mention</strong>, Argoverse Forecasting Challenge at CVPR WAD, 2023.</span></li>
  <li style="line-height: 1.2;"><span style="font-size: 16px; color: black"><strong>Best Robot Vision Paper Award</strong>, Asian Federation of Computer Vision (AFCV), 2023.</span></li>
  <li style="line-height: 1.2;"><span style="font-size: 16px; color: black"><strong>2nd place</strong>, Autonomous Driving A.I challenge (organized by MOLIT 국토교통부), 2022. </span></li>
  <li style="line-height: 1.2;"><span style="font-size: 16px; color: black"><strong>3rd place</strong>, ECCV workshop on 3D Perception for Autonomous Driving, 2022. </span></li>
  <li style="line-height: 1.2;"><span style="font-size: 16px; color: black"><strong>DGIST Presidential Fellowship</strong>, 2021. </span></li>
  <li style="line-height: 1.2;"><span style="font-size: 16px; color: black"><strong>SEOULTECH Graduate School Admission Scholarship</strong>, 2018. </span></li>
  <li style="line-height: 1.2;"><span style="font-size: 16px; color: black"><strong>Bronze Award</strong>, SoC Robot War-HURO Competition (organized by KAIST SDIA), 2018. </span></li>
  <li style="line-height: 1.2;"><span style="font-size: 16px; color: black"><strong>Bronze Award</strong>, SoC Robot War-HURO Competition (organized by KAIST SDIA), 2017. </span></li>
</ul>
-->

<div style="margin-bottom: 20px;"></div>

Reviewer
======
- IEEE International Conference on Robotics and Automation (**ICRA**)
- IEEE/RSJ International Conference on Intelligent Robots and Systems (**IROS**)
- IEEE Robotics and Automation Letters (**RA-L**)
- NeurIPS Track Datasets and Benchmarks

<!-- 
<ul style="margin: 0; padding: 0; list-style-position: inside;">
  <li style="line-height: 1.2;"><span style="font-size: 16px; color: black">IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>)</span></li>
  <li style="line-height: 1.2;"><span style="font-size: 16px; color: black">IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</span></li>
  <li style="line-height: 1.2;"><span style="font-size: 16px; color: black">IEEE Robotics and Automation Letters (<strong>RA-L</strong>)</span></li>
  <li style="line-height: 1.2;"><span style="font-size: 16px; color: black">NeurIPS Track Datasets and Benchmarks</span></li>
</ul>
-->

<!-- 
- Winner, Argoverse LiDAR Scene Flow Challenge at CVPR WAD, 2024.
- DGIST Post-Graduate Research Abroad Award, 2024.
- 1st place, Autonomous Driving A.I challenge (organized by MOLIT 국토교통부), 2023.
- Honorable Mention, Argoverse Forecasting Challenge at CVPR WAD, 2023.
- Asian Federation of Computer Vision (AFCV) Best Robot Vision Paper Award, KRoC, 2023.
- 2nd place, Autonomous Driving A.I challenge (organized by MOLIT 국토교통부), 2022. 
- 3rd place, ECCV workshop on 3D Perception for Autonomous Driving, 2022.
- DGIST Presidential Fellowship, 2021.
- SEOULTECH Graduate School Admission Scholarship, 2018.
- Bronze Award, SoC Robot War-HURO Competition (organized by KAIST SDIA), 2018.
- Bronze Award, SoC Robot War-HURO Competition (organized by KAIST SDIA), 2017.
-->

<!-- 
- IEEE International Conference on Robotics and Automation (**ICRA**)
- IEEE/RSJ International Conference on Intelligent Robots and Systems (**IROS**)
- IEEE Robotics and Automation Letters (**RA-L**)
- NeurIPS Track Datasets and Benchmarks
-->

<!-- Site-wide configuration
------
The main configuration file for the site is in the base directory in [_config.yml](https://github.com/academicpages/academicpages.github.io/blob/master/_config.yml), which defines the content in the sidebars and other site-wide features. You will need to replace the default variables with ones about yourself and your site's github repository. The configuration file for the top menu is in [_data/navigation.yml](https://github.com/academicpages/academicpages.github.io/blob/master/_data/navigation.yml). For example, if you don't have a portfolio or blog posts, you can remove those items from that navigation.yml file to remove them from the header. 

Create content & metadata
------
For site content, there is one markdown file for each type of content, which are stored in directories like _publications, _talks, _posts, _teaching, or _pages. For example, each talk is a markdown file in the [_talks directory](https://github.com/academicpages/academicpages.github.io/tree/master/_talks). At the top of each markdown file is structured data in YAML about the talk, which the theme will parse to do lots of cool stuff. The same structured data about a talk is used to generate the list of talks on the [Talks page](https://academicpages.github.io/talks), each [individual page](https://academicpages.github.io/talks/2012-03-01-talk-1) for specific talks, the talks section for the [CV page](https://academicpages.github.io/cv), and the [map of places you've given a talk](https://academicpages.github.io/talkmap.html) (if you run this [python file](https://github.com/academicpages/academicpages.github.io/blob/master/talkmap.py) or [Jupyter notebook](https://github.com/academicpages/academicpages.github.io/blob/master/talkmap.ipynb), which creates the HTML for the map based on the contents of the _talks directory).

**Markdown generator**

I have also created [a set of Jupyter notebooks](https://github.com/academicpages/academicpages.github.io/tree/master/markdown_generator
) that converts a CSV containing structured data about talks or presentations into individual markdown files that will be properly formatted for the Academic Pages template. The sample CSVs in that directory are the ones I used to create my own personal website at stuartgeiger.com. My usual workflow is that I keep a spreadsheet of my publications and talks, then run the code in these notebooks to generate the markdown files, then commit and push them to the GitHub repository.

How to edit your site's GitHub repository
------
Many people use a git client to create files on their local computer and then push them to GitHub's servers. If you are not familiar with git, you can directly edit these configuration and markdown files directly in the github.com interface. Navigate to a file (like [this one](https://github.com/academicpages/academicpages.github.io/blob/master/_talks/2012-03-01-talk-1.md) and click the pencil icon in the top right of the content preview (to the right of the "Raw | Blame | History" buttons). You can delete a file by clicking the trashcan icon to the right of the pencil icon. You can also create new files or upload files by navigating to a directory and clicking the "Create new file" or "Upload files" buttons. 

Example: editing a markdown file for a talk
![Editing a markdown file for a talk](/images/editing-talk.png)

For more info
------
More info about configuring Academic Pages can be found in [the guide](https://academicpages.github.io/markdown/). The [guides for the Minimal Mistakes theme](https://mmistakes.github.io/minimal-mistakes/docs/configuration/) (which this theme was forked from) might also be helpful. -->
